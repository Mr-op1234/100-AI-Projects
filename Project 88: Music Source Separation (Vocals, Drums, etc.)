Description:
This project separates a mixed music track into its individual components like vocals, drums, bass, and other instruments using a pretrained deep learning model such as Demucs.

Goal
Use source separation to isolate vocals or instruments from a song for karaoke, remixing, or audio analysis.

Code
import torchaudio
import torchaudio.pipelines
import soundfile as sf
import torch
 
# Load Demucs pretrained model (v4 is more accurate)
bundle = torchaudio.pipelines.DEMUCS_HTDEMUX
model = bundle.get_model()
model.eval()
 
# Load mixed audio
audio_path = "data/song_clip.wav"  # stereo WAV, 44.1kHz preferred
waveform, sample_rate = torchaudio.load(audio_path)
 
# Ensure stereo
if waveform.shape[0] == 1:
    waveform = waveform.repeat(2, 1)
 
# Perform separation
with torch.no_grad():
    sources = model(waveform.unsqueeze(0))  # shape: [1, 4, 2, T]
 
# Save separated stems
stem_names = ["drums", "bass", "other", "vocals"]
output_dir = "./outputs/separated_stems"
os.makedirs(output_dir, exist_ok=True)
 
for i, name in enumerate(stem_names):
    stem = sources[0, i]  # [2, T]
    sf.write(f"{output_dir}/{name}.wav", stem.T.cpu().numpy(), samplerate=sample_rate)
 
print("ðŸŽ¼ Separated sources saved to:", output_dir)
