Description:
This project fine-tunes DistilBERT, a lightweight version of BERT, for a binary text classification task (e.g., sentiment analysis) using Hugging Face Transformers and the IMDb dataset.

Goal
Use DistilBERT to classify movie reviews as positive or negative with high efficiency.

Code
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import torch
 
# Load IMDb dataset (subset for speed)
dataset = load_dataset("imdb")
train_dataset = dataset["train"].shuffle(seed=42).select(range(2000))
test_dataset = dataset["test"].select(range(500))
 
# Load tokenizer and model
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
 
# Tokenization
def preprocess(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)
 
train_dataset = train_dataset.map(preprocess, batched=True)
test_dataset = test_dataset.map(preprocess, batched=True)
 
# Set format for PyTorch
train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
 
# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
)
 
# Trainer API
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)
 
# Train
trainer.train()
 
# Evaluate
eval_result = trainer.evaluate()
print(f"\nEvaluation Accuracy: {eval_result['eval_accuracy']:.2f}")
